# Advanced Snowflake Tasks Implementation Guide

This document outlines 5 advanced Snowflake tasks that leverage different platform capabilities to extract maximum value from our HVAC business data. Each task focuses on specific Snowflake features while solving real business problems.

---

## üéØ Task 1: Dynamic Customer Segmentation with ML (Snowflake ML Functions)

### Business Objective
Create intelligent customer segments using Snowflake's built-in machine learning functions to identify high-value customers, at-risk accounts, and growth opportunities.

### Snowflake Features Used
- **Snowflake ML Functions**: `ML.CLUSTERING`, `ML.ANOMALY_DETECTION`
- **Window Functions**: Advanced analytics for customer metrics
- **User-Defined Functions (UDFs)**: Custom scoring logic
- **Streams**: Real-time segment updates

### Implementation Details

#### Step 1: Customer Feature Engineering
```sql
-- Create comprehensive customer feature table
CREATE OR REPLACE VIEW customer_features AS
SELECT 
    c.customer_id,
    c.customer_type,
    -- Recency, Frequency, Monetary (RFM) Analysis
    DATEDIFF('day', MAX(sc.service_date), CURRENT_DATE()) as days_since_last_service,
    COUNT(DISTINCT sc.service_call_id) as total_service_calls,
    SUM(sc.total_cost) as lifetime_value,
    AVG(sc.total_cost) as avg_service_cost,
    
    -- Service patterns
    COUNT(DISTINCT CASE WHEN sc.service_type = 'Emergency' THEN sc.service_call_id END) as emergency_calls,
    COUNT(DISTINCT CASE WHEN sc.service_type = 'Maintenance' THEN sc.service_call_id END) as maintenance_calls,
    
    -- Seasonal patterns
    COUNT(DISTINCT CASE WHEN MONTH(sc.service_date) IN (6,7,8) THEN sc.service_call_id END) as summer_calls,
    COUNT(DISTINCT CASE WHEN MONTH(sc.service_date) IN (12,1,2) THEN sc.service_call_id END) as winter_calls,
    
    -- Payment behavior
    AVG(DATEDIFF('day', i.issue_date, p.payment_date)) as avg_payment_delay,
    
    -- Satisfaction metrics
    AVG(cf.overall_satisfaction) as avg_satisfaction,
    
    -- Equipment diversity
    COUNT(DISTINCT sc.equipment_id) as equipment_count
FROM customers c
LEFT JOIN service_calls sc ON c.customer_id = sc.customer_id
LEFT JOIN invoices i ON sc.service_call_id = i.service_call_id
LEFT JOIN payments p ON i.invoice_id = p.invoice_id
LEFT JOIN customer_feedback cf ON sc.service_call_id = cf.service_call_id
GROUP BY c.customer_id, c.customer_type;
```

#### Step 2: ML-Powered Clustering
```sql
-- Use Snowflake ML clustering to identify customer segments
CREATE OR REPLACE TABLE customer_segments AS
SELECT 
    customer_id,
    -- Use ML.CLUSTERING for unsupervised segmentation
    ML.CLUSTERING(
        total_service_calls, 
        lifetime_value, 
        days_since_last_service,
        avg_satisfaction,
        emergency_calls
    ) OVER() as segment_id,
    
    -- Anomaly detection for VIP/at-risk customers
    ML.ANOMALY_DETECTION(lifetime_value, total_service_calls) OVER() as anomaly_score,
    
    -- Custom segment labels
    CASE 
        WHEN lifetime_value > 10000 AND avg_satisfaction > 8 THEN 'VIP Champions'
        WHEN emergency_calls > 3 AND avg_satisfaction < 6 THEN 'High Maintenance'
        WHEN days_since_last_service > 365 THEN 'At Risk'
        WHEN total_service_calls = 1 THEN 'New Customer'
        ELSE 'Standard'
    END as business_segment
FROM customer_features;
```

#### Step 3: Real-time Segment Updates
```sql
-- Create stream for real-time updates
CREATE OR REPLACE STREAM customer_changes ON TABLE service_calls;

-- Automated procedure to update segments
CREATE OR REPLACE PROCEDURE update_customer_segments()
RETURNS STRING
LANGUAGE SQL
AS
$$
BEGIN
    -- Update segments when new service calls arrive
    MERGE INTO customer_segments cs
    USING (
        SELECT DISTINCT customer_id FROM customer_changes
    ) changes ON cs.customer_id = changes.customer_id
    WHEN MATCHED THEN UPDATE SET 
        segment_id = ML.CLUSTERING(...), -- Recalculate
        last_updated = CURRENT_TIMESTAMP();
    
    RETURN 'Segments updated successfully';
END;
$$;
```

### Business Value
- **Personalized Marketing**: Target segments with tailored campaigns
- **Resource Allocation**: Focus high-value technicians on VIP customers
- **Churn Prevention**: Proactive outreach to at-risk segments
- **Pricing Strategy**: Dynamic pricing based on customer segment

---

## üïê Task 2: Time Series Forecasting with External Data (Snowflake Data Marketplace)

### Business Objective
Build sophisticated demand forecasting models that incorporate weather data, economic indicators, and seasonal patterns to optimize staffing and inventory.

### Snowflake Features Used
- **Snowflake Marketplace**: Weather and economic data integration
- **Time Series Functions**: `FORECAST()`, seasonal decomposition
- **External Functions**: Call external ML APIs
- **Task Scheduling**: Automated model updates
- **Secure Data Sharing**: Share forecasts with partners

### Implementation Details

#### Step 1: Integrate External Data Sources
```sql
-- Subscribe to weather data from Snowflake Marketplace
-- (Weather Source or similar provider)
USE DATABASE weather_data_marketplace;

-- Create integrated demand dataset
CREATE OR REPLACE VIEW demand_with_context AS
SELECT 
    DATE_TRUNC('day', sc.service_date) as service_date,
    COUNT(*) as daily_service_calls,
    COUNT(CASE WHEN sc.service_type = 'Emergency' THEN 1 END) as emergency_calls,
    AVG(sc.total_cost) as avg_service_value,
    
    -- Weather context
    w.temperature_high,
    w.temperature_low,
    w.humidity,
    w.weather_condition,
    
    -- Economic indicators (from marketplace)
    e.consumer_confidence_index,
    e.local_employment_rate,
    
    -- Seasonal features
    DAYOFWEEK(sc.service_date) as day_of_week,
    MONTH(sc.service_date) as month,
    CASE WHEN MONTH(sc.service_date) IN (6,7,8) THEN 'Summer'
         WHEN MONTH(sc.service_date) IN (12,1,2) THEN 'Winter'
         ELSE 'Shoulder' END as season
FROM service_calls sc
LEFT JOIN weather_data_marketplace.daily_weather w ON DATE(sc.service_date) = w.date
LEFT JOIN economic_indicators e ON DATE_TRUNC('month', sc.service_date) = e.month
GROUP BY 1,6,7,8,9,10,11,12,13,14;
```

#### Step 2: Advanced Time Series Forecasting
```sql
-- Multi-variate forecasting model
CREATE OR REPLACE TABLE demand_forecast AS
SELECT 
    service_date,
    daily_service_calls,
    
    -- Snowflake native forecasting
    FORECAST(daily_service_calls, service_date, 
             temperature_high, humidity, day_of_week, month) 
    OVER (ORDER BY service_date ROWS BETWEEN 365 PRECEDING AND CURRENT ROW) as forecast_basic,
    
    -- Seasonal decomposition
    SEASONAL_DECOMPOSE(daily_service_calls) 
    OVER (ORDER BY service_date) as (trend, seasonal, residual),
    
    -- Weather impact scoring
    CASE 
        WHEN temperature_high > 85 OR temperature_high < 32 THEN 1.5
        WHEN weather_condition LIKE '%storm%' THEN 2.0
        ELSE 1.0
    END as weather_multiplier,
    
    -- Create final adjusted forecast
    forecast_basic * weather_multiplier * 
    CASE WHEN day_of_week IN (1,7) THEN 0.7 ELSE 1.0 END as final_forecast
FROM demand_with_context
ORDER BY service_date;
```

#### Step 3: Automated Model Refresh
```sql
-- Create task for daily forecast updates
CREATE OR REPLACE TASK update_demand_forecast
    WAREHOUSE = compute_wh
    SCHEDULE = 'USING CRON 0 6 * * * America/New_York'
AS
    -- Refresh forecast with latest data
    INSERT INTO demand_forecast 
    SELECT * FROM (
        -- Forecast next 30 days
        SELECT 
            DATEADD('day', seq.value, CURRENT_DATE()) as service_date,
            NULL as daily_service_calls,
            FORECAST(...) as forecast_basic,
            -- ... rest of logic
        FROM table(generator(rowcount => 30)) seq
    );

-- Enable the task
ALTER TASK update_demand_forecast RESUME;
```

### Business Value
- **Staffing Optimization**: Right-size workforce based on predicted demand
- **Inventory Planning**: Stock parts before seasonal peaks
- **Customer Communication**: Proactive scheduling during high-demand periods
- **Financial Planning**: Accurate revenue forecasting

---

## üìç Task 3: Geospatial Route Optimization (Snowflake Geospatial)

### Business Objective
Optimize technician routes and territory assignments using Snowflake's geospatial functions to reduce travel time, fuel costs, and improve customer response times.

### Snowflake Features Used
- **Geospatial Functions**: `ST_DISTANCE`, `ST_WITHIN`, `ST_BUFFER`
- **Geospatial Joins**: Spatial relationships between locations
- **JavaScript UDFs**: Complex routing algorithms
- **Materialized Views**: Pre-computed distance matrices
- **Resource Monitors**: Control compute costs for heavy spatial queries

### Implementation Details

#### Step 1: Geospatial Data Preparation
```sql
-- Convert addresses to geospatial points
CREATE OR REPLACE TABLE customer_locations AS
SELECT 
    customer_id,
    address,
    -- Use geocoding service or pre-geocoded data
    ST_POINT(longitude, latitude) as location
FROM customers_with_coordinates;

-- Create technician service areas
CREATE OR REPLACE TABLE technician_territories AS
SELECT 
    technician_id,
    technician_name,
    -- Create 20-mile radius service area around home base
    ST_BUFFER(ST_POINT(home_longitude, home_latitude), 20 * 1609.34) as service_area,
    ST_POINT(home_longitude, home_latitude) as home_location
FROM technicians_with_coordinates;
```

#### Step 2: Spatial Analysis and Assignment
```sql
-- Intelligent job assignment based on location
CREATE OR REPLACE FUNCTION assign_optimal_technician(
    service_location GEOGRAPHY,
    service_date DATE,
    required_skills ARRAY
)
RETURNS INTEGER
LANGUAGE JAVASCRIPT
AS
$$
    // JavaScript UDF for complex assignment logic
    var best_technician = null;
    var min_total_score = 999999;
    
    // Get all available technicians for the date
    var stmt = snowflake.createStatement({
        sqlText: `SELECT t.technician_id, t.home_location, t.skills,
                         COUNT(sc.service_call_id) as workload
                  FROM technicians_with_coordinates t
                  LEFT JOIN service_calls sc ON t.technician_id = sc.technician_id 
                                              AND sc.service_date = ?
                  WHERE ST_DWITHIN(t.home_location, ?, 50000)
                  GROUP BY t.technician_id, t.home_location, t.skills`,
        binds: [SERVICE_DATE, SERVICE_LOCATION]
    });
    
    var result_set = stmt.execute();
    while (result_set.next()) {
        var distance = calculate_distance(SERVICE_LOCATION, result_set.getColumnValue('HOME_LOCATION'));
        var workload_penalty = result_set.getColumnValue('WORKLOAD') * 10;
        var skill_match = calculate_skill_match(REQUIRED_SKILLS, result_set.getColumnValue('SKILLS'));
        
        var total_score = distance + workload_penalty - skill_match;
        
        if (total_score < min_total_score) {
            min_total_score = total_score;
            best_technician = result_set.getColumnValue('TECHNICIAN_ID');
        }
    }
    
    return best_technician;
$$;
```

#### Step 3: Route Optimization
```sql
-- Daily route optimization
CREATE OR REPLACE TABLE optimized_routes AS
WITH daily_assignments AS (
    SELECT 
        technician_id,
        service_call_id,
        customer_location,
        estimated_duration_hours,
        priority_score,
        -- Calculate travel time between consecutive jobs
        LAG(customer_location) OVER (
            PARTITION BY technician_id 
            ORDER BY priority_score DESC
        ) as previous_location
    FROM todays_service_calls
),
route_segments AS (
    SELECT 
        *,
        ST_DISTANCE(previous_location, customer_location) / 1609.34 as travel_miles,
        -- Estimate travel time (assuming 30 mph average)
        (ST_DISTANCE(previous_location, customer_location) / 1609.34) / 30.0 as travel_hours
    FROM daily_assignments
)
SELECT 
    technician_id,
    ARRAY_AGG(service_call_id ORDER BY priority_score DESC) as optimized_route,
    SUM(travel_miles) as total_travel_miles,
    SUM(travel_hours + estimated_duration_hours) as total_work_hours,
    -- Calculate route efficiency score
    COUNT(*) / NULLIF(SUM(travel_hours), 0) as jobs_per_travel_hour
FROM route_segments
GROUP BY technician_id;
```

### Business Value
- **Cost Reduction**: 15-25% reduction in fuel and travel costs
- **Response Time**: Faster emergency response through optimal positioning
- **Customer Satisfaction**: More accurate arrival time estimates
- **Technician Efficiency**: More jobs per day with less driving

---

## üìä Task 4: Real-time Operational Dashboard (Snowflake Streams & Tasks)

### Business Objective
Create a real-time operational command center that tracks technician locations, job progress, customer satisfaction, and business KPIs with live updates.

### Snowflake Features Used
- **Streams**: Change data capture for real-time updates
- **Tasks**: Automated data processing pipelines
- **Materialized Views**: Fast query performance
- **Row-level Security**: Secure access to sensitive data
- **Query Acceleration Service**: Sub-second dashboard performance

### Implementation Details

#### Step 1: Real-time Data Capture
```sql
-- Create streams on all operational tables
CREATE OR REPLACE STREAM service_calls_stream ON TABLE service_calls;
CREATE OR REPLACE STREAM technician_updates_stream ON TABLE technician_locations;
CREATE OR REPLACE STREAM customer_feedback_stream ON TABLE customer_feedback;

-- Real-time KPI aggregation table
CREATE OR REPLACE TABLE real_time_kpis (
    metric_name STRING,
    metric_value FLOAT,
    metric_date TIMESTAMP_NTZ,
    granularity STRING -- 'hourly', 'daily', 'real_time'
);
```

#### Step 2: Automated Real-time Processing
```sql
-- Task to process service call updates
CREATE OR REPLACE TASK process_service_updates
    WAREHOUSE = compute_wh
    SCHEDULE = '1 MINUTE'
WHEN 
    SYSTEM$STREAM_HAS_DATA('service_calls_stream')
AS
    -- Update real-time metrics
    INSERT INTO real_time_kpis
    SELECT 
        'active_service_calls' as metric_name,
        COUNT(*) as metric_value,
        CURRENT_TIMESTAMP() as metric_date,
        'real_time' as granularity
    FROM service_calls 
    WHERE service_date = CURRENT_DATE() 
    AND status IN ('In Progress', 'Dispatched');
    
    -- Update technician utilization
    INSERT INTO real_time_kpis
    SELECT 
        'avg_technician_utilization' as metric_name,
        AVG(CASE WHEN status = 'Busy' THEN 1.0 ELSE 0.0 END) * 100 as metric_value,
        CURRENT_TIMESTAMP() as metric_date,
        'real_time' as granularity
    FROM technicians;
    
    -- Process customer satisfaction changes
    MERGE INTO real_time_kpis rt
    USING (
        SELECT 
            'avg_daily_satisfaction' as metric_name,
            AVG(overall_satisfaction) as metric_value,
            CURRENT_TIMESTAMP() as metric_date
        FROM customer_feedback 
        WHERE feedback_date = CURRENT_DATE()
    ) new_data ON rt.metric_name = new_data.metric_name 
                 AND DATE(rt.metric_date) = CURRENT_DATE()
    WHEN MATCHED THEN UPDATE SET 
        metric_value = new_data.metric_value,
        metric_date = new_data.metric_date
    WHEN NOT MATCHED THEN INSERT VALUES (
        new_data.metric_name, 
        new_data.metric_value, 
        new_data.metric_date, 
        'daily'
    );

-- Enable the task
ALTER TASK process_service_updates RESUME;
```

#### Step 3: Performance-Optimized Views
```sql
-- Materialized view for dashboard performance
CREATE OR REPLACE MATERIALIZED VIEW dashboard_summary AS
SELECT 
    -- Current operational status
    COUNT(CASE WHEN sc.status = 'In Progress' THEN 1 END) as active_jobs,
    COUNT(CASE WHEN sc.status = 'Completed' AND sc.service_date = CURRENT_DATE() THEN 1 END) as completed_today,
    COUNT(CASE WHEN sc.service_type = 'Emergency' AND sc.service_date = CURRENT_DATE() THEN 1 END) as emergency_calls_today,
    
    -- Financial metrics
    SUM(CASE WHEN sc.service_date = CURRENT_DATE() THEN sc.total_cost ELSE 0 END) as revenue_today,
    SUM(CASE WHEN DATE_TRUNC('month', sc.service_date) = DATE_TRUNC('month', CURRENT_DATE()) 
             THEN sc.total_cost ELSE 0 END) as revenue_mtd,
    
    -- Customer satisfaction
    AVG(CASE WHEN cf.feedback_date >= CURRENT_DATE() - 7 THEN cf.overall_satisfaction END) as satisfaction_7day,
    
    -- Technician metrics
    COUNT(DISTINCT CASE WHEN t.status = 'Available' THEN t.technician_id END) as available_technicians,
    AVG(CASE WHEN sc.service_date = CURRENT_DATE() THEN sc.duration_hours END) as avg_job_duration_today
FROM service_calls sc
CROSS JOIN technicians t
LEFT JOIN customer_feedback cf ON sc.service_call_id = cf.service_call_id;

-- Row-level security for different user roles
CREATE OR REPLACE ROW ACCESS POLICY technician_data_policy AS (user_role) RETURNS BOOLEAN ->
    CASE 
        WHEN CURRENT_ROLE() = 'MANAGER' THEN TRUE
        WHEN CURRENT_ROLE() = 'TECHNICIAN' AND technician_id = CURRENT_USER() THEN TRUE
        WHEN CURRENT_ROLE() = 'DISPATCHER' THEN TRUE
        ELSE FALSE
    END;

ALTER TABLE service_calls ADD ROW ACCESS POLICY technician_data_policy ON (technician_id);
```

### Business Value
- **Operational Visibility**: Real-time view of all business operations
- **Faster Decision Making**: Immediate alerts on performance issues
- **Resource Management**: Dynamic allocation based on current demand
- **Customer Service**: Proactive communication about service delays

---

## üîç Task 5: Advanced Text Analytics on Service Notes (Snowflake Cortex AI)

### Business Objective
Extract actionable insights from unstructured technician service notes using Snowflake's AI/ML capabilities to identify patterns, predict issues, and improve service quality.

### Snowflake Features Used
- **Snowflake Cortex AI**: LLM functions for text analysis
- **Vector Similarity Search**: Find similar service issues
- **Text Classification**: Automatically categorize service problems
- **Sentiment Analysis**: Gauge customer satisfaction from notes
- **Document AI**: Extract structured data from forms

### Implementation Details

#### Step 1: Text Preprocessing and Classification
```sql
-- Enhance service calls with AI-powered text analysis
CREATE OR REPLACE TABLE enhanced_service_calls AS
SELECT 
    sc.*,
    -- Extract key information using Cortex AI
    SNOWFLAKE.CORTEX.EXTRACT_ANSWER(
        sc.service_notes, 
        'What was the main problem with the HVAC system?'
    ) as primary_issue,
    
    SNOWFLAKE.CORTEX.EXTRACT_ANSWER(
        sc.service_notes, 
        'What parts were replaced or repaired?'
    ) as parts_mentioned,
    
    -- Classify service complexity
    SNOWFLAKE.CORTEX.CLASSIFY(
        sc.service_notes,
        ['Simple', 'Moderate', 'Complex', 'Requires Specialist']
    ) as complexity_level,
    
    -- Sentiment analysis of customer interaction
    SNOWFLAKE.CORTEX.SENTIMENT(sc.service_notes) as customer_interaction_sentiment,
    
    -- Extract technical details
    SNOWFLAKE.CORTEX.EXTRACT_ANSWER(
        sc.service_notes,
        'Were there any safety concerns mentioned?'
    ) as safety_concerns,
    
    -- Generate summary
    SNOWFLAKE.CORTEX.SUMMARIZE(sc.service_notes) as service_summary
FROM service_calls sc
WHERE sc.service_notes IS NOT NULL;
```

#### Step 2: Vector Similarity and Pattern Detection
```sql
-- Create embeddings for similarity search
CREATE OR REPLACE TABLE service_embeddings AS
SELECT 
    service_call_id,
    service_notes,
    primary_issue,
    equipment_id,
    -- Create vector embeddings
    SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2', service_notes) as note_embedding,
    SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2', primary_issue) as issue_embedding
FROM enhanced_service_calls;

-- Function to find similar service issues
CREATE OR REPLACE FUNCTION find_similar_issues(
    input_text STRING,
    similarity_threshold FLOAT DEFAULT 0.8
)
RETURNS TABLE (
    service_call_id INTEGER,
    similarity_score FLOAT,
    primary_issue STRING,
    service_notes STRING
)
LANGUAGE SQL
AS
$$
    SELECT 
        service_call_id,
        VECTOR_COSINE_SIMILARITY(
            SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2', input_text),
            issue_embedding
        ) as similarity_score,
        primary_issue,
        service_notes
    FROM service_embeddings
    WHERE similarity_score > similarity_threshold
    ORDER BY similarity_score DESC
    LIMIT 10
$$;
```

#### Step 3: Predictive Issue Detection
```sql
-- Identify recurring issues and predict future problems
CREATE OR REPLACE VIEW issue_patterns AS
SELECT 
    equipment_id,
    primary_issue,
    COUNT(*) as issue_frequency,
    AVG(DATEDIFF('day', LAG(service_date) OVER (
        PARTITION BY equipment_id, primary_issue 
        ORDER BY service_date
    ), service_date)) as avg_days_between_issues,
    
    -- Predict next occurrence
    DATEADD('day', 
        AVG(DATEDIFF('day', LAG(service_date) OVER (
            PARTITION BY equipment_id, primary_issue 
            ORDER BY service_date
        ), service_date)),
        MAX(service_date)
    ) as predicted_next_issue_date,
    
    -- Calculate reliability score
    CASE 
        WHEN AVG(DATEDIFF('day', LAG(service_date) OVER (
            PARTITION BY equipment_id, primary_issue 
            ORDER BY service_date
        ), service_date)) < 90 THEN 'High Risk'
        WHEN AVG(DATEDIFF('day', LAG(service_date) OVER (
            PARTITION BY equipment_id, primary_issue 
            ORDER BY service_date
        ), service_date)) < 365 THEN 'Medium Risk'
        ELSE 'Low Risk'
    END as equipment_risk_level,
    
    -- Most effective solutions
    MODE(parts_mentioned) as most_common_solution
FROM enhanced_service_calls
WHERE primary_issue IS NOT NULL
GROUP BY equipment_id, primary_issue
HAVING COUNT(*) >= 2
ORDER BY issue_frequency DESC;
```

#### Step 4: Automated Insights and Alerts
```sql
-- Create procedure for automated insights
CREATE OR REPLACE PROCEDURE generate_service_insights()
RETURNS STRING
LANGUAGE SQL
AS
$$
BEGIN
    -- Find trending issues
    CREATE OR REPLACE TEMPORARY TABLE trending_issues AS
    SELECT 
        primary_issue,
        COUNT(*) as recent_occurrences,
        COUNT(*) / LAG(COUNT(*)) OVER (ORDER BY COUNT(*)) - 1 as growth_rate
    FROM enhanced_service_calls 
    WHERE service_date >= CURRENT_DATE() - 30
    GROUP BY primary_issue
    ORDER BY growth_rate DESC;
    
    -- Generate automated recommendations
    CREATE OR REPLACE TABLE service_recommendations AS
    SELECT 
        CURRENT_TIMESTAMP() as generated_at,
        'Equipment Maintenance Alert' as recommendation_type,
        'Equipment ID ' || equipment_id || ' shows pattern of recurring ' || 
        primary_issue || ' issues. Consider proactive maintenance.' as recommendation,
        predicted_next_issue_date as action_date,
        equipment_risk_level as priority
    FROM issue_patterns
    WHERE predicted_next_issue_date <= CURRENT_DATE() + 30
    AND equipment_risk_level = 'High Risk';
    
    -- Customer communication suggestions
    INSERT INTO service_recommendations
    SELECT 
        CURRENT_TIMESTAMP() as generated_at,
        'Customer Communication' as recommendation_type,
        SNOWFLAKE.CORTEX.COMPLETE(
            'mistral-large',
            'Based on this service pattern: ' || primary_issue || 
            ' occurring every ' || avg_days_between_issues || 
            ' days, write a professional email to the customer suggesting ' ||
            'a preventive maintenance plan. Keep it under 200 words.'
        ) as recommendation,
        CURRENT_DATE() + 7 as action_date,
        'Medium' as priority
    FROM issue_patterns
    WHERE equipment_risk_level IN ('High Risk', 'Medium Risk');
    
    RETURN 'Insights generated successfully';
END;
$$;

-- Schedule automated insights generation
CREATE OR REPLACE TASK generate_weekly_insights
    WAREHOUSE = compute_wh
    SCHEDULE = 'USING CRON 0 9 * * 1 America/New_York' -- Monday 9 AM
AS
    CALL generate_service_insights();

ALTER TASK generate_weekly_insights RESUME;
```

### Business Value
- **Proactive Maintenance**: Predict equipment failures before they happen
- **Knowledge Management**: Capture and share technician expertise
- **Quality Improvement**: Identify training needs and process gaps
- **Customer Experience**: Faster issue resolution through pattern recognition
- **Cost Reduction**: Prevent expensive emergency repairs through early detection

---

## üéØ Implementation Priority and Success Metrics

### Recommended Implementation Order
1. **Customer Segmentation** (Immediate business impact, foundational for other tasks)
2. **Real-time Dashboard** (Operational efficiency gains)
3. **Geospatial Optimization** (Clear cost savings)
4. **Text Analytics** (Long-term strategic value)
5. **Demand Forecasting** (Most complex, requires external data)

### Success Metrics by Task
- **Segmentation**: 20% improvement in marketing campaign ROI
- **Forecasting**: 15% reduction in overstaffing/understaffing incidents
- **Geospatial**: 25% reduction in travel costs and time
- **Real-time Dashboard**: 50% faster issue resolution time
- **Text Analytics**: 30% reduction in repeat service calls

### Cost Management
- Set up **Resource Monitors** for each task
- Use **Query Acceleration** selectively for dashboard queries
- Implement **Automatic Clustering** on large tables
- Monitor **Credit Usage** daily during implementation

---

*Each task leverages different Snowflake capabilities while solving real business problems. Start with Customer Segmentation for immediate impact, then build complexity with each subsequent task.*